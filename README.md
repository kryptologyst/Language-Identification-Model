# Language Identification Model

A modern, state-of-the-art language identification system that can detect the language of any given text using machine learning. This project demonstrates advanced ML techniques including ensemble methods, neural networks, and provides both web UI and REST API interfaces.

## Features

- **Accurate Detection**: Identifies 14+ languages with high accuracy
- **Multiple Models**: Ensemble of Logistic Regression, Random Forest, and Neural Network
- **Confidence Scores**: Provides confidence levels for predictions
- **Web Interface**: Beautiful Streamlit app for interactive use
- **REST API**: FastAPI backend for integration with other applications
- **Batch Processing**: Analyze multiple texts simultaneously
- **File Upload**: Support for CSV file uploads
- **Analytics**: Comprehensive model performance metrics and visualizations

## Supported Languages

- English
- French
- Spanish
- German
- Italian
- Portuguese
- Japanese
- Korean
- Russian
- Hindi
- Chinese (Simplified)
- Arabic
- Dutch
- Swedish

## Quick Start

### Prerequisites

- Python 3.8+
- pip

### Installation

1. **Clone the repository**
   ```bash
   git clone <repository-url>
   cd language-identification-model
   ```

2. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

3. **Run the Streamlit web app**
   ```bash
   streamlit run app.py
   ```

4. **Or start the FastAPI server**
   ```bash
   python api.py
   ```

## Usage

### Web Interface (Streamlit)

1. Start the app: `streamlit run app.py`
2. Open your browser to `http://localhost:8501`
3. Enter text in the text area
4. Click "Detect Language" to get results

**Features:**
- Single text analysis
- Batch text processing
- CSV file upload
- Model performance visualization
- Interactive confidence gauges

### REST API (FastAPI)

1. Start the API server: `python api.py`
2. Access the API at `http://localhost:8000`
3. View interactive docs at `http://localhost:8000/docs`

**Key Endpoints:**

- `POST /predict` - Detect language of single text
- `POST /predict/batch` - Detect languages of multiple texts
- `POST /predict/file` - Upload CSV file for batch processing
- `GET /info` - Get model information
- `GET /evaluate` - Get model performance metrics
- `GET /examples` - Get example texts in different languages

**Example API Usage:**

```python
import requests

# Single text prediction
response = requests.post("http://localhost:8000/predict", json={
    "text": "Hello, how are you?",
    "confidence_threshold": 0.7
})
result = response.json()
print(f"Language: {result['predicted_language']}")
print(f"Confidence: {result['confidence']:.2%}")
```

### Python Library

```python
from language_identifier import LanguageIdentifier

# Initialize and train model
identifier = LanguageIdentifier()
df = identifier.load_mock_database()
identifier.train_models(df)

# Predict language
result = identifier.predict("Bonjour, comment allez-vous?")
print(f"Language: {result['predicted_language']}")
print(f"Confidence: {result['confidence']:.2%}")

# Batch prediction
texts = ["Hello", "Hola", "Bonjour"]
results = identifier.batch_predict(texts)
for text, result in zip(texts, results):
    print(f"{text}: {result['predicted_language']}")
```

## Architecture

### Model Architecture

The system uses an ensemble approach combining multiple machine learning models:

1. **Logistic Regression**: Fast baseline model with good interpretability
2. **Random Forest**: Robust ensemble method handling non-linear patterns
3. **Neural Network**: Multi-layer perceptron for complex pattern recognition

### Feature Engineering

- **Character-level TF-IDF**: Robust to short texts and typos
- **N-gram features**: Captures character patterns (1-4 grams)
- **Normalization**: Handles different text lengths and formats

### Technology Stack

- **Backend**: Python, scikit-learn, FastAPI
- **Frontend**: Streamlit, Plotly
- **Data Processing**: pandas, numpy
- **Visualization**: matplotlib, seaborn, plotly
- **Optional**: PyTorch, Transformers (for advanced models)

## Performance

The ensemble model achieves high accuracy across all supported languages:

- **Overall Accuracy**: >95% on test set
- **Confidence Threshold**: Configurable (default: 70%)
- **Processing Speed**: <1 second per prediction
- **Memory Usage**: Efficient, suitable for production

## Testing

Run the test suite:

```bash
pytest tests/ -v
```

Run with coverage:

```bash
pytest tests/ --cov=. --cov-report=html
```

## ðŸ“ Project Structure

```
language-identification-model/
â”œâ”€â”€ language_identifier.py    # Core ML model and classes
â”œâ”€â”€ app.py                   # Streamlit web application
â”œâ”€â”€ api.py                   # FastAPI REST API
â”œâ”€â”€ 0101.py                  # Original implementation
â”œâ”€â”€ requirements.txt         # Python dependencies
â”œâ”€â”€ README.md               # This file
â”œâ”€â”€ tests/                  # Unit tests
â”‚   â”œâ”€â”€ test_model.py
â”‚   â””â”€â”€ test_api.py
â”œâ”€â”€ data/                   # Sample datasets
â””â”€â”€ docs/                   # Documentation
```

## ðŸ”§ Configuration

### Environment Variables

Create a `.env` file for configuration:

```env
MODEL_PATH=language_identifier_model.pkl
CONFIDENCE_THRESHOLD=0.7
API_HOST=0.0.0.0
API_PORT=8000
STREAMLIT_PORT=8501
```

### Model Parameters

Key parameters you can adjust:

- `confidence_threshold`: Minimum confidence for reliable predictions
- `ngram_range`: Character n-gram range (default: 1-4)
- `max_features`: Maximum TF-IDF features (default: 5000)
- `test_size`: Train-test split ratio (default: 0.2)

## Deployment

### Docker Deployment

```dockerfile
FROM python:3.9-slim

WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt

COPY . .
EXPOSE 8000

CMD ["python", "api.py"]
```

### Cloud Deployment

The application is ready for deployment on:
- **Heroku**: Use the included `Procfile`
- **AWS**: Deploy with Elastic Beanstalk or ECS
- **Google Cloud**: Use Cloud Run or App Engine
- **Azure**: Deploy with Container Instances

## Contributing

1. Fork the repository
2. Create a feature branch: `git checkout -b feature-name`
3. Make your changes
4. Add tests for new functionality
5. Run the test suite: `pytest`
6. Commit your changes: `git commit -m "Add feature"`
7. Push to the branch: `git push origin feature-name`
8. Submit a pull request

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## Acknowledgments

- scikit-learn team for the excellent ML library
- Streamlit team for the amazing web framework
- FastAPI team for the high-performance API framework
- The open-source community for inspiration and tools

## Support

- **Issues**: Report bugs and request features on GitHub Issues
- **Discussions**: Join the community discussions
- **Documentation**: Check the `/docs` folder for detailed documentation

## Future Roadmap

- [ ] Transformer-based models (BERT, XLM-R)
- [ ] Support for 50+ languages
- [ ] Real-time language detection
- [ ] Mobile app integration
- [ ] Language confidence heatmaps
- [ ] Custom model training interface
- [ ] Multi-language text analysis
- [ ] Language detection for code comments


# Language-Identification-Model
